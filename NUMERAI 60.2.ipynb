{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load needed modules\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that returns the Cross-Validation log_loss score\n",
    "def get_log_loss(grid_search_obj):\n",
    "    print(\"Best log_loss score: {:.5f}\".format(-1 * grid_search_obj.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that uses RandomForest to return most important features\n",
    "def rf_feature_selection(df, X, y):\n",
    "    # Create a random forest\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Get the columns\n",
    "    cols = df.columns[3:-1]\n",
    "\n",
    "    # Make the forest\n",
    "    forest = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
    "\n",
    "    # Fit\n",
    "    forest.fit(X,y)\n",
    "\n",
    "    # Get the importances\n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    # Get the indices\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Show the importance of each feature\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"{} - {}\".format(cols[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that takes the probabilities and ids and saves them to a CSV file\n",
    "def save_predictions(probabilities, tourney_df, tournament_nbr=60):\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get the probability of 1\n",
    "    prob_of_one = probabilities[:, 1]\n",
    "    \n",
    "    # Get the IDs\n",
    "    ids = tourney_df['id']\n",
    "    \n",
    "    # Join the data into one DataFrame\n",
    "    results = pd.DataFrame(data={'probability': prob_of_one})\n",
    "    results = pd.DataFrame(ids).join(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results.to_csv(\"probabilities_{}.csv\".format(tournament_nbr), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>440255</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.26647</td>\n",
       "      <td>0.42487</td>\n",
       "      <td>0.81401</td>\n",
       "      <td>0.22889</td>\n",
       "      <td>0.27456</td>\n",
       "      <td>0.55654</td>\n",
       "      <td>0.55310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18847</td>\n",
       "      <td>0.77235</td>\n",
       "      <td>0.55002</td>\n",
       "      <td>0.20237</td>\n",
       "      <td>0.79605</td>\n",
       "      <td>0.82971</td>\n",
       "      <td>0.45757</td>\n",
       "      <td>0.69761</td>\n",
       "      <td>0.53739</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420205</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.41334</td>\n",
       "      <td>0.47533</td>\n",
       "      <td>0.71847</td>\n",
       "      <td>0.40792</td>\n",
       "      <td>0.32433</td>\n",
       "      <td>0.55806</td>\n",
       "      <td>0.59592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32083</td>\n",
       "      <td>0.72435</td>\n",
       "      <td>0.63751</td>\n",
       "      <td>0.29143</td>\n",
       "      <td>0.67860</td>\n",
       "      <td>0.70083</td>\n",
       "      <td>0.59967</td>\n",
       "      <td>0.53103</td>\n",
       "      <td>0.47446</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>409239</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.48937</td>\n",
       "      <td>0.56030</td>\n",
       "      <td>0.59150</td>\n",
       "      <td>0.46432</td>\n",
       "      <td>0.42291</td>\n",
       "      <td>0.54177</td>\n",
       "      <td>0.53542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.42195</td>\n",
       "      <td>0.62651</td>\n",
       "      <td>0.51604</td>\n",
       "      <td>0.42938</td>\n",
       "      <td>0.56744</td>\n",
       "      <td>0.60008</td>\n",
       "      <td>0.46966</td>\n",
       "      <td>0.50322</td>\n",
       "      <td>0.42803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>448661</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.61195</td>\n",
       "      <td>0.65958</td>\n",
       "      <td>0.45877</td>\n",
       "      <td>0.56730</td>\n",
       "      <td>0.51889</td>\n",
       "      <td>0.45049</td>\n",
       "      <td>0.56030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54803</td>\n",
       "      <td>0.59120</td>\n",
       "      <td>0.58160</td>\n",
       "      <td>0.51828</td>\n",
       "      <td>0.43870</td>\n",
       "      <td>0.47011</td>\n",
       "      <td>0.56007</td>\n",
       "      <td>0.36374</td>\n",
       "      <td>0.31552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>399213</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.43758</td>\n",
       "      <td>0.50085</td>\n",
       "      <td>0.60446</td>\n",
       "      <td>0.46663</td>\n",
       "      <td>0.47157</td>\n",
       "      <td>0.59667</td>\n",
       "      <td>0.40161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40535</td>\n",
       "      <td>0.54366</td>\n",
       "      <td>0.44763</td>\n",
       "      <td>0.37668</td>\n",
       "      <td>0.59931</td>\n",
       "      <td>0.59539</td>\n",
       "      <td>0.43771</td>\n",
       "      <td>0.54767</td>\n",
       "      <td>0.43742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   era data_type  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0  440255  era1     train   0.26647   0.42487   0.81401   0.22889   0.27456   \n",
       "1  420205  era1     train   0.41334   0.47533   0.71847   0.40792   0.32433   \n",
       "2  409239  era1     train   0.48937   0.56030   0.59150   0.46432   0.42291   \n",
       "3  448661  era1     train   0.61195   0.65958   0.45877   0.56730   0.51889   \n",
       "4  399213  era1     train   0.43758   0.50085   0.60446   0.46663   0.47157   \n",
       "\n",
       "   feature6  feature7   ...    feature13  feature14  feature15  feature16  \\\n",
       "0   0.55654   0.55310   ...      0.18847    0.77235    0.55002    0.20237   \n",
       "1   0.55806   0.59592   ...      0.32083    0.72435    0.63751    0.29143   \n",
       "2   0.54177   0.53542   ...      0.42195    0.62651    0.51604    0.42938   \n",
       "3   0.45049   0.56030   ...      0.54803    0.59120    0.58160    0.51828   \n",
       "4   0.59667   0.40161   ...      0.40535    0.54366    0.44763    0.37668   \n",
       "\n",
       "   feature17  feature18  feature19  feature20  feature21  target  \n",
       "0    0.79605    0.82971    0.45757    0.69761    0.53739       1  \n",
       "1    0.67860    0.70083    0.59967    0.53103    0.47446       1  \n",
       "2    0.56744    0.60008    0.46966    0.50322    0.42803       1  \n",
       "3    0.43870    0.47011    0.56007    0.36374    0.31552       1  \n",
       "4    0.59931    0.59539    0.43771    0.54767    0.43742       1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "# File path\n",
    "train_path = '/Users/taylordye/Desktop/numerai_datasets/numerai_training_data.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>...</th>\n",
       "      <th>feature13</th>\n",
       "      <th>feature14</th>\n",
       "      <th>feature15</th>\n",
       "      <th>feature16</th>\n",
       "      <th>feature17</th>\n",
       "      <th>feature18</th>\n",
       "      <th>feature19</th>\n",
       "      <th>feature20</th>\n",
       "      <th>feature21</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>446551</td>\n",
       "      <td>era97</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.54177</td>\n",
       "      <td>0.64267</td>\n",
       "      <td>0.54365</td>\n",
       "      <td>0.53625</td>\n",
       "      <td>0.43622</td>\n",
       "      <td>0.49010</td>\n",
       "      <td>0.59626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.46999</td>\n",
       "      <td>0.65848</td>\n",
       "      <td>0.61414</td>\n",
       "      <td>0.46340</td>\n",
       "      <td>0.51415</td>\n",
       "      <td>0.54882</td>\n",
       "      <td>0.54992</td>\n",
       "      <td>0.39919</td>\n",
       "      <td>0.33837</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>384542</td>\n",
       "      <td>era97</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.46513</td>\n",
       "      <td>0.55879</td>\n",
       "      <td>0.61386</td>\n",
       "      <td>0.48131</td>\n",
       "      <td>0.39930</td>\n",
       "      <td>0.41525</td>\n",
       "      <td>0.54916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39465</td>\n",
       "      <td>0.66286</td>\n",
       "      <td>0.58364</td>\n",
       "      <td>0.38549</td>\n",
       "      <td>0.59305</td>\n",
       "      <td>0.61357</td>\n",
       "      <td>0.52784</td>\n",
       "      <td>0.46704</td>\n",
       "      <td>0.37416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352413</td>\n",
       "      <td>era97</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.57906</td>\n",
       "      <td>0.57836</td>\n",
       "      <td>0.55062</td>\n",
       "      <td>0.50582</td>\n",
       "      <td>0.42531</td>\n",
       "      <td>0.53203</td>\n",
       "      <td>0.61312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48369</td>\n",
       "      <td>0.63943</td>\n",
       "      <td>0.56807</td>\n",
       "      <td>0.48388</td>\n",
       "      <td>0.50785</td>\n",
       "      <td>0.55630</td>\n",
       "      <td>0.55099</td>\n",
       "      <td>0.43983</td>\n",
       "      <td>0.43469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>397163</td>\n",
       "      <td>era97</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.49406</td>\n",
       "      <td>0.57476</td>\n",
       "      <td>0.59178</td>\n",
       "      <td>0.50592</td>\n",
       "      <td>0.44062</td>\n",
       "      <td>0.54272</td>\n",
       "      <td>0.55000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.43125</td>\n",
       "      <td>0.64981</td>\n",
       "      <td>0.64190</td>\n",
       "      <td>0.37122</td>\n",
       "      <td>0.56728</td>\n",
       "      <td>0.57653</td>\n",
       "      <td>0.61470</td>\n",
       "      <td>0.42992</td>\n",
       "      <td>0.36342</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>483655</td>\n",
       "      <td>era97</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.59719</td>\n",
       "      <td>0.45569</td>\n",
       "      <td>0.55065</td>\n",
       "      <td>0.41892</td>\n",
       "      <td>0.45174</td>\n",
       "      <td>0.47165</td>\n",
       "      <td>0.56128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48639</td>\n",
       "      <td>0.57764</td>\n",
       "      <td>0.45624</td>\n",
       "      <td>0.47852</td>\n",
       "      <td>0.51380</td>\n",
       "      <td>0.56741</td>\n",
       "      <td>0.52313</td>\n",
       "      <td>0.53296</td>\n",
       "      <td>0.55182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    era   data_type  feature1  feature2  feature3  feature4  \\\n",
       "0  446551  era97  validation   0.54177   0.64267   0.54365   0.53625   \n",
       "1  384542  era97  validation   0.46513   0.55879   0.61386   0.48131   \n",
       "2  352413  era97  validation   0.57906   0.57836   0.55062   0.50582   \n",
       "3  397163  era97  validation   0.49406   0.57476   0.59178   0.50592   \n",
       "4  483655  era97  validation   0.59719   0.45569   0.55065   0.41892   \n",
       "\n",
       "   feature5  feature6  feature7   ...    feature13  feature14  feature15  \\\n",
       "0   0.43622   0.49010   0.59626   ...      0.46999    0.65848    0.61414   \n",
       "1   0.39930   0.41525   0.54916   ...      0.39465    0.66286    0.58364   \n",
       "2   0.42531   0.53203   0.61312   ...      0.48369    0.63943    0.56807   \n",
       "3   0.44062   0.54272   0.55000   ...      0.43125    0.64981    0.64190   \n",
       "4   0.45174   0.47165   0.56128   ...      0.48639    0.57764    0.45624   \n",
       "\n",
       "   feature16  feature17  feature18  feature19  feature20  feature21  target  \n",
       "0    0.46340    0.51415    0.54882    0.54992    0.39919    0.33837     1.0  \n",
       "1    0.38549    0.59305    0.61357    0.52784    0.46704    0.37416     1.0  \n",
       "2    0.48388    0.50785    0.55630    0.55099    0.43983    0.43469     0.0  \n",
       "3    0.37122    0.56728    0.57653    0.61470    0.42992    0.36342     1.0  \n",
       "4    0.47852    0.51380    0.56741    0.52313    0.53296    0.55182     1.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tournement data\n",
    "test_path = '/Users/taylordye/Desktop/numerai_datasets/numerai_tournament_data.csv'\n",
    "tournament_df = pd.read_csv(test_path)\n",
    "tournament_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess, Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale\n",
    "X_train = train_df.loc[:, 'feature1':'feature21']\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(tournament_df.loc[:, 'feature1':'feature21']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the response/targets\n",
    "y_train = train_df.loc[:, 'target']\n",
    "y_test = tournament_df.loc[:, 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAHRCAYAAACvuin3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncp0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107f23a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.heatmap(X_train_scaled.corr(), ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of our features tend to be correlated. We should apply some dimensionality reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, since we have some collinearity, I think PCA could be good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a Pipeline to increase workflow and test how different models behave after we've performed feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the functions\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the Pipeline w/ \n",
    "pipe_lda_logit = Pipeline([('lda', LinearDiscriminantAnalysis(n_components=2)),\n",
    "                           ('logit', LogisticRegression(C=1.0, n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid search object\n",
    "logit_params = {'logit__C':[0.1, 1.0], 'logit__penalty':['l1', 'l2']}\n",
    "pipe_logit_gs = GridSearchCV(estimator=pipe_lda_logit, scoring='neg_log_loss', n_jobs=-1, param_grid=logit_params)\n",
    "\n",
    "# Get the average score\n",
    "pipe_logit_score = cross_val_score(estimator=pipe_logit_gs, X=X_train_scaled, y=y_train, scoring='neg_log_loss',\n",
    "                                  n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print score\n",
    "print('Average log-loss: {:.5f}'.format(-1* pipe_logit_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "pipe_lda_logit.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict \n",
    "pipe_logit_score = pipe_lda_logit.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "save_predictions(pipe_logit_score, tournament_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Transform our data via PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Cross-Validate\n",
    "cv_score_avg = cross_val_score(estimator=LogisticRegression(C=0.01, n_jobs=2), X=X_train_pca, y=y_train, \n",
    "                               n_jobs=-1, scoring='neg_log_loss', cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Logistic Regression w/ C=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "logit_preds = LogisticRegression(C=0.01, n_jobs=2).fit(X_train_scaled, y_train).predict_proba(X_test_scaled)\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(logit_preds, tournament_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Log-loss validation score: 0.69245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Instantiate, fit, predict, sumbit\n",
    "rfc_preds = RandomForestClassifier(n_estimators=150, n_jobs=2).fit(X_train_scaled, y_train).predict_proba(X_test_scaled)\n",
    "\n",
    "# Save and submit\n",
    "save_predictions(rfc_preds, tournament_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ensemble modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create list of tuples to feed the VotingClassifier\n",
    "ensemble = [('knn', KNeighborsClassifier(n_jobs=2)), ('logit', LogisticRegression(C=0.01, n_jobs=2)),\n",
    "            ('dt', DecisionTreeClassifier())]\n",
    "\n",
    "# Create the ensemble\n",
    "ensemble = VotingClassifier(estimators=ensemble, voting='soft', n_jobs=2).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "ensemble_preds = ensemble.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the ensemble preds\n",
    "save_predictions(ensemble_preds, tournament_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an ensemble of LogisticRegressions with the Adaptive Boosting technique\n",
    "ada_logit = AdaBoostClassifier(base_estimator=LogisticRegression(C=0.1, n_jobs=2)).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create and save ada_logit predictions\n",
    "save_predictions(ada_logit.predict_proba(X_test_scaled), tournament_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an ensemble of LogRed w/ Bootstrap aggregating method\n",
    "bag_logit = BaggingClassifier(base_estimator=LogisticRegression(n_jobs=2)).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create and save predictions\n",
    "save_predictions(bag_logit.predict_proba(X_test_scaled), tournament_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create list of tuples to feed the VotingClassifier\n",
    "ensemble = [('logit', LogisticRegression(C=0.01, n_jobs=2)), ('rfc', RandomForestClassifier(n_jobs=2))]\n",
    "\n",
    "# Create the ensemble\n",
    "ensemble = VotingClassifier(estimators=ensemble, voting='soft', n_jobs=2).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "ensemble_preds = ensemble.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our function to get the most discriminative features.\n",
    "- Appears features 1-3 seem to be relatively the most predictive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1 - 0.05410199355078408\n",
      "feature2 - 0.0516827099777748\n",
      "feature3 - 0.05034745177256093\n",
      "feature4 - 0.049617498576821045\n",
      "feature5 - 0.04943397458069636\n",
      "feature6 - 0.04924820137850855\n",
      "feature7 - 0.04899154116567237\n",
      "feature8 - 0.04858008649788203\n",
      "feature9 - 0.04793710323792653\n",
      "feature10 - 0.04762055436474153\n",
      "feature11 - 0.04759705358354533\n",
      "feature12 - 0.047473269837563024\n",
      "feature13 - 0.04714380602832054\n",
      "feature14 - 0.04686174352690778\n",
      "feature15 - 0.046240884781652794\n",
      "feature16 - 0.0453647163587661\n",
      "feature17 - 0.04533585522009422\n",
      "feature18 - 0.04465922690638842\n",
      "feature19 - 0.04406341166126276\n",
      "feature20 - 0.04395807034868314\n",
      "feature21 - 0.04374084664344762\n"
     ]
    }
   ],
   "source": [
    "rfc_features = rf_feature_selection(train_df, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use features 1-4 to build a model\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_logit = BaggingClassifier(base_estimator=LogisticRegression(n_jobs=2)).fit(X_train_scaled.iloc[:, 0:4], y_train)\n",
    "\n",
    "# Save predictions\n",
    "save_predictions(bag_logit.predict_proba(X_test_scaled.iloc[:, 0:4]), tournament_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try similar approach, except with Trees\n",
    "# Use features 1-4 to build a model\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_trees = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500).fit(X_train_scaled.iloc[:, 0:4],\n",
    "                                                                                          y_train)\n",
    "\n",
    "# Save predictions\n",
    "save_predictions(bag_trees.predict_proba(X_test_scaled.iloc[:, 0:4]), tournament_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement PCA to dimensionality reduce our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get the principle components\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Make a AdaBoost classifier w/ LogisticRegression as the base\n",
    "ada_logit = AdaBoostClassifier(base_estimator=LogisticRegression(n_jobs=2, C=0.001), n_estimators=200).fit(X_train_pca,\n",
    "                                                                                                      y_train)\n",
    "\n",
    "# Save and submit\n",
    "save_predictions(ada_logit.predict_proba(pca.transform(X_test_scaled)), tournament_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StochasticGradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Make Adaboost classifier w/ SGD Classifier\n",
    "ada_sgd = AdaBoostClassifier(base_estimator=SGDClassifier(loss='log', n_jobs=2), n_estimators=300).fit(X_train_pca,\n",
    "                                                                                                       y_train)\n",
    "\n",
    "# Save\n",
    "save_predictions(ada_sgd.predict_proba(pca.transform(X_test_scaled)), tournament_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
